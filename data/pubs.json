[
    {
        "name": "MultiScan: Scalable RGBD scanning for 3D environments with articulated objects",
        "image": "files/multiscan.png",
        "authors": [
            "Yongsen Mao",
            "Yiming Zhang",
            "Hanxiao Jiang",
            "Angel X. Chang",
            "Manolis Savva"
        ],
        "conference": "NeurIPS 2022",
        "special": "",
        "description": "We introduce MultiScan, a scalable RGBD dataset construction pipeline leveraging commodity mobile devices to scan indoor scenes with articulated objects and web-based semantic annotation interfaces to efficiently annotate object and part semantics and part mobility parameters. We use this pipeline to collect 230 scans of 108 indoor scenes containing 9458 objects and 4331 parts. The resulting MultiScan dataset provides RGBD streams with per-frame camera poses, textured 3D surface meshes, richly annotated part-level and object-level semantic labels, and part mobility parameters. We validate our dataset on instance segmentation and part mobility estimation tasks and benchmark methods for these tasks from prior work. Our experiments show that part segmentation and mobility estimation in real 3D scenes remain challenging despite recent progress in 3D object segmentation.",
        "paper": "",
        "project": "",
        "code": ""
    },
    {
        "name": "OPD: Single-view 3D Openable Part Detection",
        "image": "files/opd.png",
        "authors": [
            "Hanxiao Jiang",
            "Yongsen Mao",
            "Manolis Savva",
            "Angel X. Chang"
        ],
        "conference": "ECCV 2022",
        "special": "oral",
        "description": "We address the task of predicting what parts of an object can open and how they move when they do so. The input is a single image of an object, and as output we detect what parts of the object can open, and the motion parameters describing the articulation of each openable part. To tackle this task, we create two datasets of 3D objects: OPDSynth based on existing synthetic objects, and OPDReal based on RGBD reconstructions of real objects. We then design OPDRCNN, a neural architecture that detects openable parts and predicts their motion parameters. Our experiments show that this is a challenging task especially when considering generalization across object categories, and the limited amount of information in a single image. Our architecture outperforms baselines and prior work especially for RGB image inputs.",
        "paper": "https://arxiv.org/pdf/2203.16421.pdf",
        "project": "https://3dlg-hcvc.github.io/OPD/",
        "code": "https://github.com/3dlg-hcvc/OPD"
    },
    {
        "name": "Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges",
        "image": "files/articulated-3DHOI.png",
        "authors": [
            "Sanjay Haresh",
            "Xiaohao Sun",
            "Hanxiao Jiang",
            "Angel X. Chang",
            "Manolis Savva"
        ],
        "conference": "3DV 2022",
        "special": "",
        "description": "Human-object interactions with articulated objects are common in everyday life. Despite much progress in single-view 3D reconstruction, it is still challenging to infer an articulated 3D object model from an RGB video showing a person manipulating the object. We canonicalize the task of articulated 3D human-object interaction reconstruction from RGB video, and carry out a systematic benchmark of four methods for this task: 3D plane estimation, 3D cuboid estimation, CAD model fitting, and free-form mesh fitting. Our experiments show that all methods struggle to obtain high accuracy results even when provided ground truth information about the observed objects. We identify key factors which make the task challenging and suggest directions for future work on this challenging 3D computer vision task.",
        "paper": "https://arxiv.org/abs/2209.05612",
        "project": "https://3dlg-hcvc.github.io/3dhoi/",
        "code": "https://github.com/3dlg-hcvc/3dhoi"
    },
    {
        "name": "SAPIEN: A SimulAted Part-based Interactive ENvironment",
        "image": "files/sapien.png",
        "authors": [
            "Fanbo Xiang",
            "Yuzhe Qin",
            "Kaichun Mo",
            "Yikuan Xia",
            "Hao Zhu",
            "Fangchen Liu",
            "Minghua Liu",
            "Hanxiao Jiang",
            "Yifu Yuan",
            "He Wang",
            "Angel X. Chang",
            "Leonidas J. Guibas",
            "Hao Su"
        ],
        "conference": "CVPR 2020",
        "special": "oral",
        "description": "Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We benchmark state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.",
        "paper": "https://arxiv.org/abs/2003.08515",
        "project": "http://sapien.ucsd.edu/publication",
        "code": ""
    },
    {
        "name": "Motion Annotation Programs: A Scalable Approach to Annotating Kinematic Articulations in Large 3D Shape Collections",
        "image": "files/MAP.png",
        "authors": [
            "Xianghao Xu",
            "David Charatan",
            "Sonia Raychaudhuri",
            "Hanxiao Jiang",
            "Mae Heitmann",
            "Vladimir Kim",
            "Siddhartha Chaudhuri",
            "Manolis Savva",
            "Angel X. Chang",
            "Daniel Ritchie"
        ],
        "conference": "3DV 2020",
        "special": "",
        "description": "We present a system that helps individual expert users rapidly annotate kinematic motions in large 3D shape collections. The organizing concept of our system is motion annotation programs: simple, re-usable procedural rules that generate motion for a given input shape.",
        "paper": "http://visual.cs.brown.edu/projects/articulations-webpage/articulations_3dv2020.pdf",
        "project": "https://visual.cs.brown.edu/articulations",
        "code": ""
    },
    {
        "name": "Evaluating Colour Constancy on the new MIST dataset of Multi-Illuminant Scenes",
        "image": "files/color_constancy.png",
        "authors": [
            "Xiangpeng Hao",
            "Brian Funt",
            "Hanxiao Jiang"
        ],
        "conference": "CIC 2019",
        "special": "oral",
        "description": "A new image test set of synthetically generated, full-spectrum images with pixelwise ground truth has been developed to aid in the evaluation of illumination estimation methods for colour constancy. The performance of 9 illumination methods is reported for this dataset along and compared to the optimal single-illuminant estimate. None of the methods specifically designed to handle multi-illuminant scenes is found to perform any better than the optimal single-illuminant case based on completely uniform illumination.",
        "paper": "https://www.ingentaconnect.com/contentone/ist/cic/2019/00002019/00000001/art00021?crawler=true&mimetype=application/pdf",
        "project": "",
        "code": ""
    }
    
]
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <!-- Use the .htaccess and remove these lines to avoid edge case issues.
     More info: h5bp.com/i/378 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

  <meta name="google-site-verification" content="s9HfQLacjwqz4eRF6EoYjc99aE5T00xyFcCA3zIniwc" />

  <!-- Our site title and description -->
<title>Hanxiao Jiang(Shawn) | 蒋含啸</title>

  <meta name="generator" content="DocPad v6.79.4" />

  <!-- Mobile viewport optimized: h5bp.com/viewport -->
  <meta name="viewport" content="width=device-width" />

  <!-- Shims: IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script async src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <link rel="stylesheet" href="/styles/twitter-bootstrap.css" />
  <link rel="stylesheet" href="/styles/style.css" />
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>
  <script src="/scripts/bootstrap.min.js"></script>
  <script src="/scripts/script.js"></script>
</head>

<body>
  <div class="container">
    <section id="content" class="content">
      <nav class="navbar navbar-default">
        <div class="container-fluid">
          <!-- Brand and toggle get grouped for better mobile display -->
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#"></a>
          </div>

          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav">
              <li><a href="index.html">Home</a></li>
              <li><a href="pubs.html">Publications</a></li>
              <li><a href="news.html">News</a></li>
              <li><a href="experience.html">Experience</a></li>
              <li><a href="contact.html">Contact</a></li>
            </ul>
          </div><!-- /.navbar-collapse -->
        </div><!-- /.container-fluid -->
      </nav>

      <div class="row">
<div style="float: right">
    <audio controls autoplay loop height="50" width="50">
        <source src="music/Rauf_Faik_Kolybel_naja.mp3" type="audio/mpeg">
        <embed height="50" width="100" src="music/Rauf_Faik_Kolybel_naja.mp3">
    </audio>
</div>

<h1>Hanxiao Jiang(Shawn) | 蒋含啸</h1>
<h5>为天地立心，为生民立命。 为往圣继绝学，为万世开太平。 --- 《横渠语录》 </h5>
<h5>To ordain conscience for Heaven and Earth. To secure life and fortune for the people. To continue lost teachings for past sages. To establish peace for all future generations.</h5>
        <hr>
        <div class="col-sm-8 pull-left" style="min-height: 280px; display: flex; flex-direction: column;">
<p>
    I'm a thesis-based master student of computer science at <a href="https://www.sfu.ca">Simon Fraser University</a>, instructed by professor 
    <a href="https://angelxuanchang.github.io">Angel Xuan Chang</a>. Prior to this, I received my Bachelor of Engineer & Bachelor of Science from <a
    href="https://www.zju.edu.cn/english/">Zhejiang University</a> and <a href="https://www.sfu.ca">Simon
    Fraser University</a>.
    Currently I am also a research assistant in <a href="https://gruvi.cs.sfu.ca/people/">SFU Gruiv Group</a> working with professor <a href="https://angelxuanchang.github.io">Angel
      Xuan Chang</a> and professor <a href="https://msavva.github.io">Manolis Savva</a> at <a
      href="https://www.sfu.ca">Simon Fraser University</a>.
    My research interests are 3D vision and robotics vision. (Going to apply for 2023 Fall PhD)
</p>
<p>
<h3>News</h3>
<ul>
<li> Jan, 2023 - Got Helmut & Hugo Eppich Family Grad Scholarship for 2023 Spring, <a href="https://www.sfu.ca">Simon Fraser University</a>. </li>
<li> Sep, 2022 - One paper accepted at <a href="https://nips.cc/">NeurIPS 2022</a>.</li>
<li> Aug, 2022 - Student Volunteer at <a href="https://s2022.siggraph.org/">SIGGRAPH 2022</a>.</li>
<li> Aug, 2022 - One paper accepted at <a href="https://3dvconf.github.io/2022/">3DV 2022</a>.</li>
<li> Jul, 2022 - My first first-author paper, <a href="https://3dlg-hcvc.github.io/OPD/" target="_blank">OPD</a>, gets accepted to <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a> as Oral.</li>
<li> Dec, 2021 - Got Helmut & Hugo Eppich Family Grad Scholarship for 2022 Spring, <a href="https://www.sfu.ca">Simon Fraser University</a>. </li>
</ul>
<a href="news.html">More...</a>
</p>
        </div>
        <div class="col-sm-4 text-right pull-right">
<img src="files/hanxiao.jpg" alt="Hanxiao Jiang" width="150px" />
<div style="font-family:monospace;">
shawn_jiang-{at}-sfu-"dot"-ca
</div><br />

Thesis-based Master Student<br />
Sep 2020 - Present<br />
<a href="https://www.sfu.ca">Simon Fraser University</a><br />
Research Assistant (<a href="https://gruvi.cs.sfu.ca/people/">Gruiv</a> & <a href="https://angelxuanchang.github.io/group.html">3dlg</a>)<br />
<a href="https://scholar.google.com/citations?user=-XWZKZAAAAAJ&hl=en">Google Scholar</a><br />
        </div>
      </div>
      <br>

<div class="row">
    <div class="col-sm-12">
      <div class="panel panel-default">
        <div class="panel-heading">
          <h2>Publications</h2>
        </div>
        <div class="panel-body">

<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/multiscan.png alt="Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">MultiScan: Scalable RGBD scanning for 3D environments with articulated objects</h4>
<a href=https://github.com/SamMaoYS target="_blank">Yongsen Mao</a>,
<a href=https://github.com/eamonn-zh target="_blank">Yiming Zhang</a>,
<strong>Hanxiao Jiang</strong>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a><br>
<a href=https://nips.cc/ target="_blank">NeurIPS 2022</a>
<p>We introduce MultiScan, a scalable RGBD dataset construction pipeline leveraging commodity mobile devices to scan indoor scenes with articulated objects and web-based semantic annotation interfaces to efficiently annotate object and part semantics and part mobility parameters. We use this pipeline to collect 230 scans of 108 indoor scenes containing 9458 objects and 4331 parts. The resulting MultiScan dataset provides RGBD streams with per-frame camera poses, textured 3D surface meshes, richly annotated part-level and object-level semantic labels, and part mobility parameters. We validate our dataset on instance segmentation and part mobility estimation tasks and benchmark methods for these tasks from prior work. Our experiments show that part segmentation and mobility estimation in real 3D scenes remain challenging despite recent progress in 3D object segmentation.</p>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/opd.png alt="Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">OPD: Single-view 3D Openable Part Detection</h4>
<strong>Hanxiao Jiang</strong>,
<a href=https://github.com/SamMaoYS target="_blank">Yongsen Mao</a>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a><br>
<a href=https://eccv2022.ecva.net/ target="_blank">ECCV 2022</a>, <i style="color: red">Oral Presentation</i>
<p>We address the task of predicting what parts of an object can open and how they move when they do so. The input is a single image of an object, and as output we detect what parts of the object can open, and the motion parameters describing the articulation of each openable part. To tackle this task, we create two datasets of 3D objects: OPDSynth based on existing synthetic objects, and OPDReal based on RGBD reconstructions of real objects. We then design OPDRCNN, a neural architecture that detects openable parts and predicts their motion parameters. Our experiments show that this is a challenging task especially when considering generalization across object categories, and the limited amount of information in a single image. Our architecture outperforms baselines and prior work especially for RGB image inputs.</p>
<a href=https://arxiv.org/pdf/2203.16421.pdf target="_blank">[Paper]</a>
<a href=https://3dlg-hcvc.github.io/OPD/ target="_blank">[Project]</a>
<a href=https://github.com/3dlg-hcvc/OPD target="_blank">[Code]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/articulated-3DHOI.png alt="Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges</h4>
<a href=https://www.sanjayharesh.com/ target="_blank">Sanjay Haresh</a>,
<a href=https://ca.linkedin.com/in/xiaohao-sun-237537195?trk=public_profile_browsemap target="_blank">Xiaohao Sun</a>,
<strong>Hanxiao Jiang</strong>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a><br>
<a href=https://3dvconf.github.io/2022/ target="_blank">3DV 2022</a>
<p>Human-object interactions with articulated objects are common in everyday life. Despite much progress in single-view 3D reconstruction, it is still challenging to infer an articulated 3D object model from an RGB video showing a person manipulating the object. We canonicalize the task of articulated 3D human-object interaction reconstruction from RGB video, and carry out a systematic benchmark of four methods for this task: 3D plane estimation, 3D cuboid estimation, CAD model fitting, and free-form mesh fitting. Our experiments show that all methods struggle to obtain high accuracy results even when provided ground truth information about the observed objects. We identify key factors which make the task challenging and suggest directions for future work on this challenging 3D computer vision task.</p>
<a href=https://arxiv.org/abs/2209.05612 target="_blank">[Paper]</a>
<a href=https://3dlg-hcvc.github.io/3dhoi/ target="_blank">[Project]</a>
<a href=https://github.com/3dlg-hcvc/3dhoi target="_blank">[Code]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/sapien.png alt="Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">SAPIEN: A SimulAted Part-based Interactive ENvironment</h4>
<a href=https://www.fbxiang.com/ target="_blank">Fanbo Xiang</a>,
<a href=https://yzqin.github.io/ target="_blank">Yuzhe Qin</a>,
<a href=https://cs.stanford.edu/~kaichun/ target="_blank">Kaichun Mo</a>,
<a href=https://www.linkedin.com/in/yikuan-xia-4418a9170/ target="_blank">Yikuan Xia</a>,
<a href=https://berniezhu.github.io/ target="_blank">Hao Zhu</a>,
<a href=https://fangchenliu.github.io/ target="_blank">Fangchen Liu</a>,
<a href=http://cseweb.ucsd.edu/~mil070/ target="_blank">Minghua Liu</a>,
<strong>Hanxiao Jiang</strong>,
Yifu Yuan,
<a href=http://ai.stanford.edu/~hewang/ target="_blank">He Wang</a>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
<a href=http://geometry.stanford.edu/member/guibas/ target="_blank">Leonidas J. Guibas</a>,
<a href=http://cseweb.ucsd.edu/~haosu/ target="_blank">Hao Su</a><br>
<a href=http://cvpr2020.thecvf.com/ target="_blank">CVPR 2020</a>, <i style="color: red">Oral Presentation</i>
<p>Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We benchmark state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.</p>
<a href=https://arxiv.org/abs/2003.08515 target="_blank">[Paper]</a>
<a href=http://sapien.ucsd.edu/publication target="_blank">[Project]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/MAP.png alt="Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">Motion Annotation Programs: A Scalable Approach to Annotating Kinematic Articulations in Large 3D Shape Collections</h4>
<a href=https://www.linkedin.com/in/xianghao-xu-8b1024a6 target="_blank">Xianghao Xu</a>,
<a href=http://davidcharatan.com/ target="_blank">David Charatan</a>,
<a href=https://ca.linkedin.com/in/sonia-raychaudhuri target="_blank">Sonia Raychaudhuri</a>,
<strong>Hanxiao Jiang</strong>,
<a href=https://www.linkedin.com/in/mae-heitmann-770287118 target="_blank">Mae Heitmann</a>,
<a href=http://www.vovakim.com/ target="_blank">Vladimir Kim</a>,
<a href=https://www.cse.iitb.ac.in/~sidch/ target="_blank">Siddhartha Chaudhuri</a>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
<a href=https://dritchie.github.io/ target="_blank">Daniel Ritchie</a><br>
<a href=http://3dv2020.dgcv.nii.ac.jp/ target="_blank">3DV 2020</a>
<p>We present a system that helps individual expert users rapidly annotate kinematic motions in large 3D shape collections. The organizing concept of our system is motion annotation programs: simple, re-usable procedural rules that generate motion for a given input shape.</p>
<a href=http://visual.cs.brown.edu/projects/articulations-webpage/articulations_3dv2020.pdf target="_blank">[Paper]</a>
<a href=https://visual.cs.brown.edu/articulations target="_blank">[Project]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/color_constancy.png alt="Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">Evaluating Colour Constancy on the new MIST dataset of Multi-Illuminant Scenes</h4>
<a href=https://www.haoxp.xyz/#/ target="_blank">Xiangpeng Hao</a>,
<a href=https://www2.cs.sfu.ca/~funt/ target="_blank">Brian Funt</a>,
<strong>Hanxiao Jiang</strong><br>
<a href=https://www.imaging.org/site/IST/Conferences/Color_and_Imaging/CIC27__2019_/IST/Conferences/CIC/CIC2019/CIC27_Home.aspx?hkey=0fb3eb64-061f-4609-8d3c-7b8e6c0c4b90 target="_blank">CIC 2019</a>, <i style="color: red">Oral Presentation</i>
<p>A new image test set of synthetically generated, full-spectrum images with pixelwise ground truth has been developed to aid in the evaluation of illumination estimation methods for colour constancy. The performance of 9 illumination methods is reported for this dataset along and compared to the optimal single-illuminant estimate. None of the methods specifically designed to handle multi-illuminant scenes is found to perform any better than the optimal single-illuminant case based on completely uniform illumination.</p>
<a href=https://www.ingentaconnect.com/contentone/ist/cic/2019/00002019/00000001/art00021?crawler=true&mimetype=application/pdf target="_blank">[Paper]</a>
    </div>
  </div>
<br>

        </div>
      </div>
    </div>

  </div>
  </div>
  </section>
  <!-- <footer>
    <p class="pull-right">
      Last updated at 2020-10-07T10:56:51.321Z
    </p>
  </footer> -->
  </div><!-- /container -->
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-162763877-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-162763877-1');
  </script>

</body>

</html>
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <!-- Use the .htaccess and remove these lines to avoid edge case issues.
     More info: h5bp.com/i/378 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

  <meta name="google-site-verification" content="s9HfQLacjwqz4eRF6EoYjc99aE5T00xyFcCA3zIniwc" />

  <!-- Our site title and description -->
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<title>Hanxiao Jiang(Shawn) | 蒋含啸</title>

  <meta name="generator" content="DocPad v6.79.4" />

  <!-- Mobile viewport optimized: h5bp.com/viewport -->
  <meta name="viewport" content="width=device-width" />

  <!-- Shims: IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script async src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <link rel="stylesheet" href="/styles/twitter-bootstrap.css" />
  <link rel="stylesheet" href="/styles/style.css" />
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>
  <script src="/scripts/bootstrap.min.js"></script>
  <script src="/scripts/script.js"></script>
</head>

<body>
  <div class="container">
    <section id="content" class="content">
      <nav class="navbar navbar-default">
        <div class="container-fluid">
          <!-- Brand and toggle get grouped for better mobile display -->
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#"></a>
          </div>

          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav">
              <li><a href="index.html">Home</a></li>
              <li><a href="pubs.html">Publications</a></li>
              <li><a href="news.html">News</a></li>
              <li><a href="experience.html">Experience</a></li>
              <li><a href="contact.html">Contact</a></li>
            </ul>
          </div><!-- /.navbar-collapse -->
        </div><!-- /.container-fluid -->
      </nav>

      <div class="row">
<div style="float: right">
    <audio controls autoplay loop height="50" width="50">
        <source src="music/yiliaobailiao.mp3" type="audio/mpeg">
        <embed height="50" width="100" src="music/yiliaobailiao.mp3">
    </audio>
</div>
<img src=files/name.png alt="image" class="img-responsive" style="width: 700px;" />
<!-- <h1>Hanxiao Jiang(Shawn) | 蒋含啸</h1> -->
<h5>为天地立心，为生民立命。 为往圣继绝学，为万世开太平。 --- 《横渠语录》 </h5>
<h5>To ordain conscience for Heaven and Earth. To secure life and fortune for the people. To continue lost teachings for past sages. To establish peace for all future generations.</h5>
        <hr>
        <div class="col-sm-8 pull-left" style="min-height: 280px; display: flex; flex-direction: column;">
<p>
   I'm a Ph.D. student in Computer Science at <a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a>, 
   co-advised by Prof. <a href="https://yunzhuli.github.io/">Yunzhu Li</a> and Prof. <a href="https://shenlong.web.illinois.edu/">Shenlong Wang</a>. 
   My research interests lie in the intersection of 3D Vision and Robotics Learning.
   <br/>
   <br/>

   I received my master degree in Computer Science at <a href="https://www.sfu.ca">Simon Fraser University</a>, fortunately advised by Prof. 
    <a href="https://angelxuanchang.github.io">Angel Xuan Chang</a> and Prof. <a href="https://msavva.github.io">Manolis Savva</a>. 
    Prior to this, I received my Bachelor of Engineer & Bachelor of Science from the dual degree program between  <a
    href="https://www.zju.edu.cn/english/">Zhejiang University</a> and <a href="https://www.sfu.ca">Simon
    Fraser University</a>.
</p>
<p>
<h3>News</h3>
<ul>
<li> Jun, 2023 - My thesis can be viewed <a href="https://summit.sfu.ca/item/36177">here</a>.</li>
<li> March, 2023 - Successfully defended my MSc thesis!</li>
<li> March, 2023 - My new journey will be a CS PHD at <a href="https://illinois.edu/">UIUC</a> starting from 2023 Fall, working with Prof <a href="https://yunzhuli.github.io/">Yunzhu Li</a> and Prof <a href="https://shenlong.web.illinois.edu/">Shenlong Wang</a></li>
<li> Jan, 2023 - Got Helmut & Hugo Eppich Family Grad Scholarship for 2023 Spring, <a href="https://www.sfu.ca">Simon Fraser University</a>. </li>
<li> Sep, 2022 - One paper accepted at <a href="https://nips.cc/">NeurIPS 2022</a>.</li>
<li> Aug, 2022 - Student Volunteer at <a href="https://s2022.siggraph.org/">SIGGRAPH 2022</a>.</li>
</ul>
<a href="news.html">More...</a>
</p>
        </div>
        <div class="col-sm-4 text-right pull-right">
<img src="files/hanxiao.jpg" alt="Hanxiao Jiang" width="150px" />
<div style="font-family:monospace;">
shawn_jiang-{at}-sfu-"dot"-ca
</div><br />

CS Ph.D. Student<br />
Aug 2023 - Present<br />
<a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a><br />
<!-- Research Assistant (<a href="https://gruvi.cs.sfu.ca/people/">Gruiv</a> & <a href="https://angelxuanchang.github.io/group.html">3dlg</a>)<br /> -->
<a href="https://scholar.google.com/citations?user=-XWZKZAAAAAJ&hl=en">Google Scholar</a><br />
        </div>
      </div>
      <br>

<div class="row">
    <div class="col-sm-12">
      <div class="panel panel-default">
        <div class="panel-heading">
          <h2>Preprints & Publications</h2>
        </div>
        <div class="panel-body">

<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/hssd.png alt="image" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation</h4>
<a href= https://mukulkhanna.github.io/ target="_blank">Mukul Khanna*</a>,
<a href=https://sammaoys.github.io/ target="_blank">Yongsen Mao*</a>,
<strong>Hanxiao Jiang</strong>,
<a href=https://www.sanjayharesh.com/ target="_blank">Sanjay Haresh</a>,
<a href= https://cs.stanford.edu/~bps/ target="_blank">Brennan Shacklett</a>,
<a href= https://faculty.cc.gatech.edu/~dbatra/ target="_blank">Dhruv Batra</a>,
<a href= https://www.linkedin.com/in/alexander-clegg-68336839/ target="_blank">Alexander Clegg</a>,
<a href= https://www.linkedin.com/in/ericu/ target="_blank">Eric Undersander</a>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a><br>
<a href=https://arxiv.org/ target="_blank">arXiv</a>
<p>We contribute the Habitat Synthetic Scenes Dataset (HSSD-200), a dataset of 211 high-quality 3D scenes, and use it to test navigation agent generalization to realistic 3D environments. Our dataset represents real interiors and contains a diverse set of 18,656 models of real-world objects. We investigate the impact of synthetic 3D scene dataset scale and realism on the task of training embodied agents to find and navigate to objects (ObjectGoal navigation). By comparing to synthetic 3D scene datasets from prior work, we find that scale helps in generalization, but the benefits quickly saturate, making visual fidelity and correlation to real-world scenes more important. Our experiments show that agents trained on our smaller-scale dataset can match or outperform agents trained on much larger datasets. Surprisingly, we observe that agents trained on just 122 scenes from our dataset outperform agents trained on 10,000 scenes from the ProcTHOR-10K dataset in terms of zero-shot generalization in real-world scanned environments.</p>
<a href=https://arxiv.org/abs/2306.11290 target="_blank">[Paper]</a>
<a href=https://3dlg-hcvc.github.io/hssd/ target="_blank">[Project]</a>
<a href=https://github.com/3dlg-hcvc/hssd/ target="_blank">[Code]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/opdmulti.png alt="image" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">OPDMulti: Openable Part Detection for Multiple Objects</h4>
<a href=https://sun-xh.github.io/ target="_blank">Xiaohao Sun*</a>,
<strong>Hanxiao Jiang*</strong>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a><br>
<a href=https://arxiv.org/ target="_blank">arXiv</a>
<p>Openable part detection is the task of detecting the openable parts of an object in a single-view image, and predicting corresponding motion parameters. Prior work investigated the unrealistic setting where all input images only contain a single openable object. We generalize this task to scenes with multiple objects each potentially possessing openable parts, and create a corresponding dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a part-aware transformer architecture. Our experiments show that the OPDFormer architecture significantly outperforms prior work. The more realistic multiple-object scenarios we investigated remain challenging for all methods, indicating opportunities for future work</p>
<a href=https://arxiv.org/pdf/2303.14087.pdf target="_blank">[Paper]</a>
<a href=https://3dlg-hcvc.github.io/OPDMulti/ target="_blank">[Project]</a>
<a href=https://github.com/3dlg-hcvc/OPDMulti target="_blank">[Code]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/multiscan.png alt="image" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">MultiScan: Scalable RGBD scanning for 3D environments with articulated objects</h4>
<a href=https://sammaoys.github.io/ target="_blank">Yongsen Mao</a>,
<a href=https://github.com/eamonn-zh target="_blank">Yiming Zhang</a>,
<strong>Hanxiao Jiang</strong>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a><br>
<a href=https://nips.cc/ target="_blank">NeurIPS 2022</a>
<p>We introduce MultiScan, a scalable RGBD dataset construction pipeline leveraging commodity mobile devices to scan indoor scenes with articulated objects and web-based semantic annotation interfaces to efficiently annotate object and part semantics and part mobility parameters. We use this pipeline to collect 230 scans of 108 indoor scenes containing 9458 objects and 4331 parts. The resulting MultiScan dataset provides RGBD streams with per-frame camera poses, textured 3D surface meshes, richly annotated part-level and object-level semantic labels, and part mobility parameters. We validate our dataset on instance segmentation and part mobility estimation tasks and benchmark methods for these tasks from prior work. Our experiments show that part segmentation and mobility estimation in real 3D scenes remain challenging despite recent progress in 3D object segmentation.</p>
<a href=https://openreview.net/pdf?id=YxUdazpgweG target="_blank">[Paper]</a>
<a href=https://3dlg-hcvc.github.io/multiscan/#/ target="_blank">[Project]</a>
<a href=https://github.com/smartscenes/multiscan target="_blank">[Code]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/opd.png alt="image" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">OPD: Single-view 3D Openable Part Detection</h4>
<strong>Hanxiao Jiang</strong>,
<a href=https://sammaoys.github.io/ target="_blank">Yongsen Mao</a>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a><br>
<a href=https://eccv2022.ecva.net/ target="_blank">ECCV 2022</a>, <i style="color: red">Oral Presentation</i>
<p>We address the task of predicting what parts of an object can open and how they move when they do so. The input is a single image of an object, and as output we detect what parts of the object can open, and the motion parameters describing the articulation of each openable part. To tackle this task, we create two datasets of 3D objects: OPDSynth based on existing synthetic objects, and OPDReal based on RGBD reconstructions of real objects. We then design OPDRCNN, a neural architecture that detects openable parts and predicts their motion parameters. Our experiments show that this is a challenging task especially when considering generalization across object categories, and the limited amount of information in a single image. Our architecture outperforms baselines and prior work especially for RGB image inputs.</p>
<a href=https://arxiv.org/pdf/2203.16421.pdf target="_blank">[Paper]</a>
<a href=https://3dlg-hcvc.github.io/OPD/ target="_blank">[Project]</a>
<a href=https://github.com/3dlg-hcvc/OPD target="_blank">[Code]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/articulated-3DHOI.png alt="image" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges</h4>
<a href=https://www.sanjayharesh.com/ target="_blank">Sanjay Haresh</a>,
<a href=https://sun-xh.github.io/ target="_blank">Xiaohao Sun</a>,
<strong>Hanxiao Jiang</strong>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a><br>
<a href=https://3dvconf.github.io/2022/ target="_blank">3DV 2022</a>
<p>Human-object interactions with articulated objects are common in everyday life. Despite much progress in single-view 3D reconstruction, it is still challenging to infer an articulated 3D object model from an RGB video showing a person manipulating the object. We canonicalize the task of articulated 3D human-object interaction reconstruction from RGB video, and carry out a systematic benchmark of four methods for this task: 3D plane estimation, 3D cuboid estimation, CAD model fitting, and free-form mesh fitting. Our experiments show that all methods struggle to obtain high accuracy results even when provided ground truth information about the observed objects. We identify key factors which make the task challenging and suggest directions for future work on this challenging 3D computer vision task.</p>
<a href=https://arxiv.org/abs/2209.05612 target="_blank">[Paper]</a>
<a href=https://3dlg-hcvc.github.io/3dhoi/ target="_blank">[Project]</a>
<a href=https://github.com/3dlg-hcvc/3dhoi target="_blank">[Code]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/sapien.png alt="image" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">SAPIEN: A SimulAted Part-based Interactive ENvironment</h4>
<a href=https://www.fbxiang.com/ target="_blank">Fanbo Xiang</a>,
<a href=https://yzqin.github.io/ target="_blank">Yuzhe Qin</a>,
<a href=https://cs.stanford.edu/~kaichun/ target="_blank">Kaichun Mo</a>,
<a href=https://www.linkedin.com/in/yikuan-xia-4418a9170/ target="_blank">Yikuan Xia</a>,
<a href=https://berniezhu.github.io/ target="_blank">Hao Zhu</a>,
<a href=https://fangchenliu.github.io/ target="_blank">Fangchen Liu</a>,
<a href=http://cseweb.ucsd.edu/~mil070/ target="_blank">Minghua Liu</a>,
<strong>Hanxiao Jiang</strong>,
Yifu Yuan,
<a href=http://ai.stanford.edu/~hewang/ target="_blank">He Wang</a>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
<a href=http://geometry.stanford.edu/member/guibas/ target="_blank">Leonidas J. Guibas</a>,
<a href=http://cseweb.ucsd.edu/~haosu/ target="_blank">Hao Su</a><br>
<a href=http://cvpr2020.thecvf.com/ target="_blank">CVPR 2020</a>, <i style="color: red">Oral Presentation</i>
<p>Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We benchmark state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.</p>
<a href=https://arxiv.org/abs/2003.08515 target="_blank">[Paper]</a>
<a href=http://sapien.ucsd.edu/publication target="_blank">[Project]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/MAP.png alt="image" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">Motion Annotation Programs: A Scalable Approach to Annotating Kinematic Articulations in Large 3D Shape Collections</h4>
<a href=https://www.linkedin.com/in/xianghao-xu-8b1024a6 target="_blank">Xianghao Xu</a>,
<a href=http://davidcharatan.com/ target="_blank">David Charatan</a>,
<a href=https://ca.linkedin.com/in/sonia-raychaudhuri target="_blank">Sonia Raychaudhuri</a>,
<strong>Hanxiao Jiang</strong>,
<a href=https://www.linkedin.com/in/mae-heitmann-770287118 target="_blank">Mae Heitmann</a>,
<a href=http://www.vovakim.com/ target="_blank">Vladimir Kim</a>,
<a href=https://www.cse.iitb.ac.in/~sidch/ target="_blank">Siddhartha Chaudhuri</a>,
<a href=https://msavva.github.io/ target="_blank">Manolis Savva</a>,
<a href=https://angelxuanchang.github.io/ target="_blank">Angel X. Chang</a>,
<a href=https://dritchie.github.io/ target="_blank">Daniel Ritchie</a><br>
<a href=http://3dv2020.dgcv.nii.ac.jp/ target="_blank">3DV 2020</a>
<p>3D models of real-world objects are essential for many applications, including the creation of virtual environments for AI training. To mimic real-world objects in these applications, objects must be annotated with their kinematic mobilities. Annotating kinematic motions is time-consuming, and it is not well-suited to typical crowdsourcing workflows due to the significant domain expertise required. In this paper, we present a system that helps individual expert users rapidly annotate kinematic motions in large 3D shape collections. The organizing concept of our system is motion annotation programs: simple, re-usable procedural rules that generate motion for a given input shape. Our interactive system allows users to author these rules and quickly apply them to collections of functionally-related objects. Using our system, an expert annotated over 1000 joints in under 3 hours. In a user study, participants with no prior experience with our system were able to annotate motions 1.5x faster than with a baseline manual annotation tool.</p>
<a href=http://visual.cs.brown.edu/projects/articulations-webpage/articulations_3dv2020.pdf target="_blank">[Paper]</a>
<a href=https://visual.cs.brown.edu/articulations target="_blank">[Project]</a>
    </div>
  </div>
<br>
<div class="row paper vertical-center" id="pub_1" data-maintags="vision,conference">
    <div class="col-sm-3 paper-img">
<a href="#"><img src=files/color_constancy.png alt="image" class="img-responsive" style="width: 300px;" /></a>
      &nbsp;
    </div>
    <div class="col-sm-9">
<h4 class="red">Evaluating Colour Constancy on the new MIST dataset of Multi-Illuminant Scenes</h4>
<a href=https://www.haoxp.xyz/#/ target="_blank">Xiangpeng Hao</a>,
<a href=https://www2.cs.sfu.ca/~funt/ target="_blank">Brian Funt</a>,
<strong>Hanxiao Jiang</strong><br>
<a href=https://www.imaging.org/site/IST/Conferences/Color_and_Imaging/CIC27__2019_/IST/Conferences/CIC/CIC2019/CIC27_Home.aspx?hkey=0fb3eb64-061f-4609-8d3c-7b8e6c0c4b90 target="_blank">CIC 2019</a>, <i style="color: red">Oral Presentation</i>
<p>A new image test set of synthetically generated, full-spectrum images with pixelwise ground truth has been developed to aid in the evaluation of illumination estimation methods for colour constancy. The performance of 9 illumination methods is reported for this dataset along and compared to the optimal single-illuminant estimate. None of the methods specifically designed to handle multi-illuminant scenes is found to perform any better than the optimal single-illuminant case based on completely uniform illumination.</p>
<a href=https://www2.cs.sfu.ca/~funt/Hao+Funt_MIST_CIC27_2019.pdf target="_blank">[Paper]</a>
    </div>
  </div>
<br>

        </div>
      </div>
    </div>

  </div>
  </div>
  </section>
  <!-- <footer>
    <p class="pull-right">
      Last updated at 2020-10-07T10:56:51.321Z
    </p>
  </footer> -->
  </div><!-- /container -->
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-162763877-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-162763877-1');
  </script>

</body>

</html>